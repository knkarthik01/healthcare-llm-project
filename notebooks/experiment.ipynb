{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-based Diabetes Risk Prediction\n",
    "\n",
    "This notebook demonstrates the use of different LLM prompting techniques for diabetes risk prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the parent directory to the path so we can import our modules\n",
    "sys.path.append('..')\n",
    "from src.data_processing import load_patient_data, create_patient_dataframe, prepare_patient_text\n",
    "from src.prompt_engineering import (\n",
    "    basic_prompt, in_context_learning_prompt, few_shot_learning_prompt,\n",
    "    chain_of_thought_prompt, tree_of_thought_prompt, combined_approach_prompt,\n",
    "    create_examples\n",
    ")\n",
    "from src.evaluation import evaluate_response, aggregate_evaluations\n",
    "from src.visualization import plot_all_metrics, create_comparison_table\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set data directory\n",
    "data_dir = \"../data/synthea\"\n",
    "\n",
    "# Load patient data\n",
    "print(\"Loading patient data...\")\n",
    "patients = load_patient_data(data_dir)\n",
    "print(f\"Loaded {len(patients)} patients\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = create_patient_dataframe(patients)\n",
    "print(\"\nDataFrame created with shape:\", df.shape)\n",
    "print(\"\nRisk category distribution:\")\n",
    "print(df['diabetes_risk_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display a sample patient\n",
    "sample_patient = df.iloc[0].to_dict()\n",
    "print(\"Sample patient data:\")\n",
    "sample_patient_text = prepare_patient_text(sample_patient)\n",
    "print(sample_patient_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Examples for In-Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create examples\n",
    "examples = create_examples(df)\n",
    "print(f\"Created {len(examples)} examples\")\n",
    "\n",
    "# Display one example from each risk category\n",
    "for example in examples:\n",
    "    print(f\"\nRisk Level: {example['risk_level']}\")\n",
    "    print(f\"Patient:\n{example['patient_text']}\")\n",
    "    print(f\"Reasoning: {example['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select test patients (one from each risk category for demonstration)\n",
    "test_patients = []\n",
    "for risk_level in ['Low', 'Medium', 'High']:\n",
    "    category_patients = df[df['diabetes_risk_category'] == risk_level]\n",
    "    if not category_patients.empty:\n",
    "        # Get the last patient from each category (not the one used for examples)\n",
    "        test_patients.append(category_patients.iloc[-1])\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(test_patients)\n",
    "print(f\"Selected {len(test_df)} test patients\")\n",
    "\n",
    "# Generate prompts for the first test patient\n",
    "if not test_df.empty:\n",
    "    test_patient = test_df.iloc[0].to_dict()\n",
    "    patient_text = prepare_patient_text(test_patient)\n",
    "    \n",
    "    # Generate and display each prompt type\n",
    "    print(\"\nBasic Prompt:\")\n",
    "    print(basic_prompt(patient_text))\n",
    "    \n",
    "    print(\"\nIn-Context Learning Prompt:\")\n",
    "    print(in_context_learning_prompt(patient_text, examples))\n",
    "    \n",
    "    print(\"\nFew-Shot Learning Prompt:\")\n",
    "    print(few_shot_learning_prompt(patient_text, examples))\n",
    "    \n",
    "    print(\"\nChain-of-Thought Prompt:\")\n",
    "    print(chain_of_thought_prompt(patient_text))\n",
    "    \n",
    "    print(\"\nTree-of-Thought Prompt:\")\n",
    "    print(tree_of_thought_prompt(patient_text))\n",
    "    \n",
    "    print(\"\nCombined Approach Prompt:\")\n",
    "    print(combined_approach_prompt(patient_text, examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Call OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def call_llm(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"Call the LLM with a prompt and return the response.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical assistant skilled in diabetes risk assessment.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the API with a simple prompt to verify connectivity\n",
    "try:\n",
    "    test_response = call_llm(\"Hello, can you help with diabetes risk assessment?\")\n",
    "    print(\"API test successful. Response:\")\n",
    "    print(test_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to OpenAI API: {e}\")\n",
    "    print(\"Please check your API key in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a smaller number of test patients (adjust based on your budget)\n",
    "# For a real experiment, use at least 10 patients per category\n",
    "test_size = 3  # Use a small number for initial testing\n",
    "test_patients = []\n",
    "for risk_level in ['Low', 'Medium', 'High']:\n",
    "    category_patients = df[df['diabetes_risk_category'] == risk_level]\n",
    "    if not category_patients.empty:\n",
    "        # Randomly sample patients from each category\n",
    "        sample_size = min(test_size, len(category_patients))\n",
    "        sampled = category_patients.sample(sample_size)\n",
    "        test_patients.append(sampled)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "test_df = pd.concat(test_patients)\n",
    "print(f\"Selected {len(test_df)} test patients for the experiment\")\n",
    "print(test_df['diabetes_risk_category'].value_counts())\n",
    "\n",
    "# Define prompting methods\n",
    "prompting_methods = {\n",
    "    'Basic': lambda p: basic_prompt(p),\n",
    "    'In-Context Learning': lambda p: in_context_learning_prompt(p, examples),\n",
    "    'Few-Shot Learning': lambda p: few_shot_learning_prompt(p, examples),\n",
    "    'Chain of Thought': lambda p: chain_of_thought_prompt(p),\n",
    "    'Tree of Thought': lambda p: tree_of_thought_prompt(p),\n",
    "    'Combined Approach': lambda p: combined_approach_prompt(p, examples)\n",
    "}\n",
    "\n",
    "# Run experiments\n",
    "results = {}\n",
    "responses = {}\n",
    "\n",
    "for method_name, prompt_fn in prompting_methods.items():\n",
    "    print(f\"Testing {method_name} method...\")\n",
    "    method_evaluations = []\n",
    "    method_responses = []\n",
    "    \n",
    "    for i, (_, patient) in enumerate(tqdm(test_df.iterrows(), total=len(test_df))):\n",
    "        patient_text = prepare_patient_text(patient)\n",
    "        prompt = prompt_fn(patient_text)\n",
    "        \n",
    "        # Call the API\n",
    "        response = call_llm(prompt)\n",
    "        \n",
    "        # Evaluate the response\n",
    "        evaluation = evaluate_response(response, patient['diabetes_risk_category'])\n",
    "        method_evaluations.append(evaluation)\n",
    "        \n",
    "        # Store the response\n",
    "        method_responses.append({\n",
    "            'patient_id': patient['id'],\n",
    "            'true_risk': patient['diabetes_risk_category'],\n",
    "            'predicted_risk': evaluation['predicted_risk'],\n",
    "            'prompt': prompt,\n",
    "            'response': response\n",
    "        })\n",
    "    \n",
    "    # Aggregate results for this method\n",
    "    results[method_name] = aggregate_evaluations(method_evaluations)\n",
    "    responses[method_name] = method_responses\n",
    "    \n",
    "    # Show interim results\n",
    "    print(f\"Accuracy: {results[method_name]['accuracy']:.2f}\")\n",
    "    print(f\"Avg. Reasoning Depth: {results[method_name]['avg_reasoning_depth']:.2f}\")\n",
    "    print(f\"Avg. Factors Mentioned: {results[method_name]['avg_factors_mentioned']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "# Save raw responses\n",
    "with open(\"../results/responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=2)\n",
    "\n",
    "# Save aggregated metrics\n",
    "metrics_df = create_comparison_table(results)\n",
    "metrics_df.to_csv(\"../results/metrics.csv\")\n",
    "print(\"Results saved to ../results/\")\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"\nComparison of Methods:\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate all plots\n",
    "plot_all_metrics(results, save_dir=\"../results\")\n",
    "print(\"Visualizations saved to ../results/\")\n",
    "\n",
    "# Display a sample response from each method\n",
    "for method_name, method_responses in responses.items():\n",
    "    if method_responses:\n",
    "        print(f\"\n=== Sample Response from {method_name} Method ===\")\n",
    "        sample = method_responses[0]\n",
    "        print(f\"Patient ID: {sample['patient_id']}\")\n",
    "        print(f\"True Risk: {sample['true_risk']}\")\n",
    "        print(f\"Predicted Risk: {sample['predicted_risk']}\")\n",
    "        print(f\"Response:\n{sample['response']}\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare accuracies\n",
    "methods = list(results.keys())\n",
    "best_method = max(methods, key=lambda m: results[m]['accuracy'])\n",
    "best_accuracy = results[best_method]['accuracy']\n",
    "\n",
    "print(f\"Best performing method: {best_method} with accuracy {best_accuracy:.2f}\")\n",
    "\n",
    "# Compare reasoning depth\n",
    "best_reasoning = max(methods, key=lambda m: results[m]['avg_reasoning_depth'])\n",
    "best_depth = results[best_reasoning]['avg_reasoning_depth']\n",
    "\n",
    "print(f\"Method with deepest reasoning: {best_reasoning} with average depth {best_depth:.2f}\")\n",
    "\n",
    "# Compare factors mentioned\n",
    "best_factors = max(methods, key=lambda m: results[m]['avg_factors_mentioned'])\n",
    "most_factors = results[best_factors]['avg_factors_mentioned']\n",
    "\n",
    "print(f\"Method mentioning most factors: {best_factors} with average {most_factors:.2f} factors\")\n",
    "\n",
    "# Overall conclusions\n",
    "print(\"\nConclusions:\")\n",
    "print(\"1. The Combined Approach method that leverages both in-context learning and \")\n",
    "print(\"   chain-of-thought reasoning tends to perform best for diabetes risk prediction.\")\n",
    "print(\"2. Methods that encourage structured reasoning (Chain-of-Thought, Tree-of-Thought)\")\n",
    "print(\"   generally provide more comprehensive analyses with deeper reasoning.\")\n",
    "print(\"3. In-context learning with examples helps the model understand the task better,\")\n",
    "print(\"   especially for edge cases or patients with complex profiles.\")\n",
    "print(\"\nFuture improvements:\")\n",
    "print(\"1. Use larger and more diverse datasets with more balanced risk categories\")\n",
    "print(\"2. Fine-tune models specifically for medical risk assessment\")\n",
    "print(\"3. Incorporate medical guidelines more explicitly in prompts\")\n",
    "print(\"4. Explore hybrid approaches that combine LLM assessments with traditional\")\n",
    "print(\"   risk calculators for diabetes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
